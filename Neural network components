"""
Model components for the Multimodal Misinformation Detection System.

This module provides the neural network components used in the system, including
text encoding, image encoding, cross-modal attention, and fusion modules.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import BertModel, ViTModel
from torch_geometric.nn import GCNConv, global_mean_pool

# Text Encoder using BERT
class TextEncoder(nn.Module):
    """Text encoder using BERT."""
    
    def __init__(self, config):
        """
        Initialize the text encoder.
        
        Args:
            config: Configuration object
        """
        super(TextEncoder, self).__init__()
        self.bert = BertModel.from_pretrained(config.text_model_name)
        self.dropout = nn.Dropout(config.dropout_rate)
        
    def forward(self, input_ids, attention_mask):
        """
        Forward pass through the text encoder.
        
        Args:
            input_ids: Input token IDs [batch_size, seq_length]
            attention_mask: Attention mask [batch_size, seq_length]
            
        Returns:
            torch.Tensor: Text features [batch_size, hidden_size]
        """
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        # Using the CLS token representation as the text embedding
        cls_output = outputs.last_hidden_state[:, 0, :]
        return self.dropout(cls_output)

# Image Encoder using Vision Transformer
class ImageEncoder(nn.Module):
    """Image encoder using Vision Transformer."""
    
    def __init__(self, config):
        """
        Initialize the image encoder.
        
        Args:
            config: Configuration object
        """
        super(ImageEncoder, self).__init__()
        self.vit = ViTModel.from_pretrained(config.image_model_name)
        self.dropout = nn.Dropout(config.dropout_rate)
        
    def forward(self, pixel_values):
        """
        Forward pass through the image encoder.
        
        Args:
            pixel_values: Input images [batch_size, 3, height, width]
            
        Returns:
            torch.Tensor: Image features [batch_size, hidden_size]
        """
        outputs = self.vit(pixel_values=pixel_values)
        # Using the CLS token representation as the image embedding
        cls_output = outputs.last_hidden_state[:, 0, :]
        return self.dropout(cls_output)

# Cross-Modal Attention Mechanism
class CrossModalAttention(nn.Module):
    """Cross-modal attention mechanism for multimodal analysis."""
    
    def __init__(self, hidden_size, num_heads=8):
        """
        Initialize the cross-modal attention mechanism.
        
        Args:
            hidden_size: Size of the hidden representations
            num_heads: Number of attention heads
        """
        super(CrossModalAttention, self).__init__()
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads
        
        self.q_linear = nn.Linear(hidden_size, hidden_size)
        self.k_linear = nn.Linear(hidden_size, hidden_size)
        self.v_linear = nn.Linear(hidden_size, hidden_size)
        
        self.out_proj = nn.Linear(hidden_size, hidden_size)
        
    def forward(self, query_features, key_value_features):
        """
        Forward pass through the cross-modal attention.
        
        Args:
            query_features: Features to use as queries [batch_size, hidden_size]
            key_value_features: Features to use as keys and values [batch_size, hidden_size]
            
        Returns:
            torch.Tensor: Attention output [batch_size, hidden_size]
        """
        batch_size = query_features.size(0)
        
        # Queries
        q = self.q_linear(query_features).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        
        # Keys and values
        k = self.k_linear(key_value_features).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        v = self.v_linear(key_value_features).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        
        # Scale dot-product attention
        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)
        attn_weights = F.softmax(scores, dim=-1)
        
        # Apply attention to values
        context = torch.matmul(attn_weights, v)
        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.head_dim)
        
        # Output projection
        output = self.out_proj(context)
        
        return output.squeeze(1)

# Multimodal Fusion Module
class MultimodalFusion(nn.Module):
    """Multimodal fusion module for combining text and image features."""
    
    def __init__(self, config):
        """
        Initialize the multimodal fusion module.
        
        Args:
            config: Configuration object
        """
        super(MultimodalFusion, self).__init__()
        
        self.text_projection = nn.Linear(config.hidden_size, config.fusion_hidden_size)
        self.image_projection = nn.Linear(config.hidden_size, config.fusion_hidden_size)
        
        self.cross_attn_text_to_image = CrossModalAttention(config.fusion_hidden_size)
        self.cross_attn_image_to_text = CrossModalAttention(config.fusion_hidden_size)
        
        self.fusion_layer = nn.Sequential(
            nn.Linear(config.fusion_hidden_size * 4, config.fusion_hidden_size),
            nn.ReLU(),
            nn.Dropout(config.dropout_rate)
        )
        
    def forward(self, text_features, image_features):
        """
        Forward pass through the fusion module.
        
        Args:
            text_features: Text features [batch_size, hidden_size]
            image_features: Image features [batch_size, hidden_size]
            
        Returns:
            torch.Tensor: Fused features [batch_size, fusion_hidden_size]
        """
        # Project features to the same space
        text_proj = self.text_projection(text_features)
        image_proj = self.image_projection(image_features)
        
        # Cross-modal attention
        text_attended = self.cross_attn_text_to_image(text_proj, image_proj)
        image_attended = self.cross_attn_image_to_text(image_proj, text_proj)
        
        # Concatenate features
        concat_features = torch.cat([
            text_proj, 
            image_proj,
            text_attended,
            image_attended
        ], dim=1)
        
        # Fuse features
        fused_features = self.fusion_layer(concat_features)
        
        return fused_features

# Graph Neural Network for Coordination Detection
class CoordinationDetector(nn.Module):
    """Graph neural network for coordination pattern detection."""
    
    def __init__(self, in_channels, hidden_channels=128):
        """
        Initialize the coordination detector.
        
        Args:
            in_channels: Number of input channels
            hidden_channels: Number of hidden channels
        """
        super(CoordinationDetector, self).__init__()
        
        self.conv1 = GCNConv(in_channels, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, hidden_channels)
        
        self.linear = nn.Linear(hidden_channels, 1)
        
    def forward(self, x, edge_index, batch):
        """
        Forward pass through the coordination detector.
        
        Args:
            x: Node features [num_nodes, in_channels]
            edge_index: Graph connectivity [2, num_edges]
            batch: Batch assignment for nodes [num_nodes]
            
        Returns:
            torch.Tensor: Coordination score [batch_size, 1]
        """
        # Apply graph convolutions
        x = F.relu(self.conv1(x, edge_index))
        x = F.dropout(x, p=0.5, training=self.training)
        x = self.conv2(x, edge_index)
        
        # Global pooling
        x = global_mean_pool(x, batch)
        
        # Final prediction
        x = self.linear(x)
        
        return torch.sigmoid(x)

# Main Multimodal Misinformation Detection Model
class MisinformationDetector(nn.Module):
    """Main model for multimodal misinformation detection."""
    
    def __init__(self, config):
        """
        Initialize the misinformation detector.
        
        Args:
            config: Configuration object
        """
        super(MisinformationDetector, self).__init__()
        
        self.text_encoder = TextEncoder(config)
        self.image_encoder = ImageEncoder(config)
        self.fusion_module = MultimodalFusion(config)
        
        self.classifier = nn.Sequential(
            nn.Linear(config.fusion_hidden_size, config.fusion_hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(config.dropout_rate),
            nn.Linear(config.fusion_hidden_size // 2, config.num_classes)
        )
        
    def forward(self, input_ids, attention_mask, pixel_values):
        """
        Forward pass through the misinformation detector.
        
        Args:
            input_ids: Input token IDs [batch_size, seq_length]
            attention_mask: Attention mask [batch_size, seq_length]
            pixel_values: Input images [batch_size, 3, height, width]
            
        Returns:
            torch.Tensor: Classification logits [batch_size, num_classes]
        """
        # Extract features
        text_features = self.text_encoder(input_ids, attention_mask)
        image_features = self.image_encoder(pixel_values)
        
        # Fuse features
        fused_features = self.fusion_module(text_features, image_features)
        
        # Classify
        logits = self.classifier(fused_features)
        
        return logits
    
    def extract_features(self, input_ids, attention_mask, pixel_values):
        """
        Extract features without classification.
        
        Args:
            input_ids: Input token IDs [batch_size, seq_length]
            attention_mask: Attention mask [batch_size, seq_length]
            pixel_values: Input images [batch_size, 3, height, width]
            
        Returns:
            tuple: (text_features, image_features, fused_features)
        """
        # Extract features
        text_features = self.text_encoder(input_ids, attention_mask)
        image_features = self.image_encoder(pixel_values)
        
        # Fuse features
        fused_features = self.fusion_module(text_features, image_features)
        
        return text_features, image_features, fused_features

# Temporal Pattern Analyzer
class TemporalPatternAnalyzer(nn.Module):
    """Module for analyzing temporal patterns in content distribution."""
    
    def __init__(self, input_size, hidden_size=64, num_layers=2):
        """
        Initialize the temporal pattern analyzer.
        
        Args:
            input_size: Size of input features
            hidden_size: Size of hidden layers
            num_layers: Number of recurrent layers
        """
        super(TemporalPatternAnalyzer, self).__init__()
        
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            bidirectional=True,
            dropout=0.2 if num_layers > 1 else 0
        )
        
        self.attention = nn.Sequential(
            nn.Linear(hidden_size * 2, hidden_size),
            nn.Tanh(),
            nn.Linear(hidden_size, 1)
        )
        
        self.fc = nn.Linear(hidden_size * 2, 1)
        
    def forward(self, features, timestamps):
        """
        Forward pass through the temporal pattern analyzer.
        
        Args:
            features: Content features [batch_size, seq_length, input_size]
            timestamps: Normalized timestamps [batch_size, seq_length]
            
        Returns:
            torch.Tensor: Temporal pattern score [batch_size, 1]
        """
        # Sort by timestamps
        _, indices = timestamps.sort(dim=1)
        batch_size, seq_length = timestamps.size()
        
        # Reorder features according to timestamps
        sorted_features = torch.zeros_like(features)
        for i in range(batch_size):
            sorted_features[i] = features[i][indices[i]]
        
        # Pass through LSTM
        lstm_out, _ = self.lstm(sorted_features)
        
        # Apply attention
        attention_scores = self.attention(lstm_out).squeeze(-1)
        attention_weights = F.softmax(attention_scores, dim=1).unsqueeze(1)
        
        # Weighted sum
        context = torch.bmm(attention_weights, lstm_out).squeeze(1)
        
        # Final prediction
        output = torch.sigmoid(self.fc(context))
        
        return output

# Ensemble Model combining multiple detection strategies
class EnsembleDetector(nn.Module):
    """Ensemble model combining multiple detection strategies."""
    
    def __init__(self, config):
        """
        Initialize the ensemble detector.
        
        Args:
            config: Configuration object
        """
        super(EnsembleDetector, self).__init__()
        
        self.misinformation_detector = MisinformationDetector(config)
        self.temporal_analyzer = TemporalPatternAnalyzer(
            input_size=config.fusion_hidden_size,
            hidden_size=config.fusion_hidden_size // 2
        )
        
        # Meta-classifier to combine predictions
        self.meta_classifier = nn.Sequential(
            nn.Linear(3, 8),  # 3 inputs: misinfo score, coordination score, temporal score
            nn.ReLU(),
            nn.Linear(8, 1),
            nn.Sigmoid()
        )
        
    def forward(self, batch_data):
        """
        Forward pass through the ensemble detector.
        
        Args:
            batch_data: Dictionary containing batch data
            
        Returns:
            torch.Tensor: Final prediction [batch_size, 1]
        """
        # Extract inputs
        input_ids = batch_data['text']['input_ids']
        attention_mask = batch_data['text']['attention_mask']
        images = batch_data['image']
        timestamps = batch_data.get('timestamps', None)
        
        # Misinformation detection
        misinfo_logits = self.misinformation_detector(input_ids, attention_mask, images)
        misinfo_probs = F.softmax(misinfo_logits, dim=1)[:, 1].unsqueeze(1)  # Probability of misinformation
        
        # If temporal data is available
        if timestamps is not None:
            # Extract features
            _, _, fused_features = self.misinformation_detector.extract_features(
                input_ids, attention_mask, images
            )
            
            # Temporal analysis
            temporal_score = self.temporal_analyzer(fused_features.unsqueeze(1), timestamps.unsqueeze(1))
            
            # For demonstration, use a placeholder for coordination score
            # In a real implementation, this would come from the CoordinationDetector
            coordination_score = torch.zeros_like(misinfo_probs)
            
            # Combine predictions
            combined_input = torch.cat([misinfo_probs, coordination_score, temporal_score], dim=1)
            final_score = self.meta_classifier(combined_input)
        else:
            # If no temporal data, just use misinformation score
            final_score = misinfo_probs
        
        return final_score
